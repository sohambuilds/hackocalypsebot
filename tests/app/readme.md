# RAG Chat System - README
The RAG (Retrieval-Augmented Generation) Chat System is an AI-powered chatbot built using modern machine learning techniques, including the LangChain framework. This system leverages embeddings, a vector store (FAISS), and retrieval-augmented question answering (QA) to provide intelligent, context-aware responses based on user queries. The system processes text files, stores embeddings, and allows users to interactively query the chatbot to receive relevant answers.
## Features

- **File-based Context**: The chatbot can process and generate answers based on the contents of uploaded text files.
- **Retrieval-Augmented Generation**: Combines the power of document retrieval with generation using a pre-trained Groq-based model (`mixtral-8x7b-32768`).
- **Embeddings & Vector Store**: Utilizes embeddings generated by the HuggingFace model and stores them in a FAISS vector store for efficient similarity searches.
- **Interactive Chat System**: Users can interact with the chatbot, upload files, and ask questions via a user-friendly interface powered by Streamlit.
## Technologies Used

- **LangChain**: A framework that integrates language models, vector stores, and chains for building complex pipelines.
- **FAISS**: A library for efficient similarity search and clustering of embeddings.
- **HuggingFace**: For generating sentence embeddings with a transformer model (`sentence-transformers/all-mpnet-base-v2`).
- **Groq**: A custom large language model (`mixtral-8x7b-32768`) used for generating answers based on the context.
- **Streamlit**: For creating a web-based interactive user interface.
- **Python**: The primary programming language for the implementation.
## Components

The project consists of the following main Python scripts:

- **`RagAnswerGenerator.py`**: Contains the logic for generating answers using embeddings, a vector store, and the Groq-based model.
- **`RagChatPipelines.py`**: Manages the chatbot's interactive chat loop and allows users to ask questions.
- **`app.py`**: The Streamlit web app that handles file uploads, initializes the RAG system, and displays answers to user queries.

### `RagAnswerGenerator.py`
This module defines the `RagAnswerGenerator` class responsible for:
- Loading pre-trained embeddings using HuggingFace models.
- Splitting text files into smaller chunks.
- Creating a FAISS vector store to store document embeddings.
- Using the Groq-based LLM to generate answers based on the retrieved context.

### `RagChatPipelines.py`
This module manages the chat loop, allowing users to input questions and receive contextually relevant answers by calling the `RagAnswerGenerator`.

### `app.py`
This module contains the Streamlit app where users can:
- Upload text files for processing.
- Ask questions about the contents of the uploaded files.
- Receive answers based on the RAG pipeline.
## Setup Instructions

To run this project locally, follow these steps:

### Step 1: Clone the Repository

```bash
git clone https://github.com/your-repository-url/rag-chat-system.git
cd rag-chat-system
```

### Step 2: Set Up a Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`
```

### Step 3: Install Required Dependencies
Install the necessary Python packages:
```bash
pip install -r requirements.txt
```
**Note:** The `requirements.txt` file should include all the dependencies, including LangChain, HuggingFace, FAISS, and Streamlit.

### Step 4: Set Environment Variables

You need to set your API key for the Groq model.

Create a `.env` file or export the variable in your terminal:

```bash
GROQ_API_KEY=your_groq_api_key_here
```

Alternatively, export it directly in the terminal:

```bash
export GROQ_API_KEY=your_groq_api_key_here
```

### Step 5: Run the Application
Start the Streamlit application:

```bash
streamlit run app.py
```

## How it Works?

1. **File Upload:** Users upload a text file containing documents, which are indexed and processed into embeddings.
2. **Embeddings & Vector Store:** The text file is processed into chunks, embeddings are generated, and they are stored in a FAISS vector store for fast retrieval.
3. **Question Answering:** When users ask a question, the system retrieves relevant context from the vector store and generates an answer using the Groq-based language model.
4. **Answer Display:** The system displays the answer to the user through the Streamlit interface.
## Example Use Case

- **User Uploads a File:** The user uploads a text document containing map details.
- **User Asks a Question:** "Where are the resources located?"
- **System Retrieves Context:** The system looks through the indexed document and retrieves relevant information.
- **Response:** The system generates an answer like "Resources are located in district 4, near coordinates (x, y)."
## Project Structure

```plaintext
/rag-chat-system
    ├── app.py                  # Streamlit app to handle user interactions
    ├── RagAnswerGenerator.py    # Core logic for RAG answer generation
    ├── RagChatPipelines.py      # Chat pipeline to manage user interaction
    ├── requirements.txt         # List of dependencies
    └── /data                    # Directory to store uploaded files
```

### Key Classes and Functions:

- **RagAnswerGenerator:** Handles the generation of answers using embeddings and the Groq model.
- **RagChatPipeline:** Manages the interactive loop where the user can ask questions.
- **init_rag:** Initializes the RAG system after the file upload.
- **generate_answer:** Retrieves the most relevant context and generates the answer using the Groq model.

## Troubleshooting
- **No response or error from Groq:** Ensure your API key is correctly set in the environment variables.
- **Error while uploading files:** Ensure the file is in .txt format and try again.
- **Slow performance:** If the file is large, it may take time to process the embeddings and create the vector store.
